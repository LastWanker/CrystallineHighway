下一步工作建议（通俗版）

目前代码已经把“寻找驱动建构”的骨架跑通了，但仍有一些关键逻辑缺口需要补齐。你可以把它理解为“高速公路主体已经铺好，但收费站、维护队和交通规则还没完善”。下面是可按优先级推进的方向：

第一步，完善真实的存储层。现在的 InMemoryStore 只能在内存里跑，断电即失。下一步必须设计持久化存储（例如 SQLite、向量库或本地文件序列化），否则长期记忆无法积累。设计时要保持“词典、实例册、图结构”三者分离，持久化策略也应分三部分实现。

第二步，真正接入“后台背诵/复习调度”。目前背诵是同步触发的，而且只针对输入文本。如果检索发现“没路”，系统应该把对应内容丢给后台慢慢复习，补路、补固化、补索引。你需要设计：什么时候触发后台背诵、每次背多少、走多少轮、消耗多少预算。这是从原型走向实用的核心一步。

第三步，完善“固化元的引用与原文追溯”。目前固化元只保存了源文本的拼接字符串，但没有真正的“原文片段引用”。要实现“输出给 LLM 的是完整原文”，就必须建立原文片段索引，比如保存原文 ID + 起止位置，或者保存原始文本库与索引映射。

第四步，补全“频率与统计”体系。现在只有 private_freq/use_count/pass_count 等基础计数，缺少全局频率的接入、私人高频固化元的注册门槛、以及固化元频率上升后“允许分身”的机制。建议逐步实现：
1）接入真实语料统计作为 global_freq；
2）定义“私人高频阈值”，触发字典注册；
3）当固化元变得高频时，允许它像普通高频词一样分身。

第五步，检索排序策略需要更明确。当前排序主要依赖点亮次数与配额，但没有考虑层级优先、输出多样性、重复过滤。可以加入“层级优先 + 点亮排序 + 多样性惩罚”的组合规则，保证输出既有整体又有细节。

第六步，完善中文工具链。现在默认用 jieba，LTP 是可选接入，但还没有做强绑定。后续可以做成“插件式分词器”，并提供配置开关，让用户选择 jieba、LTP、或更智能的工具。注意不要在核心逻辑里硬编码某个工具，保持可替换。

如果你不知道从哪里开始，建议按顺序做：持久化存储 → 后台背诵调度 → 原文追溯 → 频率体系 → 检索排序 → 分词工具链。这样可以确保系统从“概念模型”逐步变成“可长期使用的记忆系统”。
