1) 固化阈值的分层规则：短语/短句/段落等不同层级是否采用不同阈值？新的指示：阈值先设置递增的，那就是2、3、4、5……
按照什么递增？按照内涵固化元数量的log2为基底向上取整再加一递增，也就是说，路径两头是2元阈值是1+1，路径两头加起来是4元阈值是2+1……（为什么递增？因为实际上walk_count会被继承）
注意，新规则：维护一个固化元数量标签，在固化的时候单纯地相加。这个暂时是绑定词典的，（我不知道会不会出现相同的句子但是固化元数量不一样，可能是有的，那就直接绑定实例吧。）

2) 固化路径的长度范围：是否允许三元、四元以上连续路径触发固化？
固化路径的长度范围：不允许2以上的连续路径触发固化。实际上，新增规则，触发固化后如果继续搜寻，不立刻在已经固化的基础上再次触发固化，具体表现为：如果目前现状是A1B1C1D，输入依旧为ABCD，就会这样，A2B触发固化，AB形成,，但是不用管，把AB的前后输入输出工作维护好就行，A2B因为固化，回归到A1B（固化后原路径置1），原链路现在是A1B1C1D，继续走BC，但是由于B刚刚产参与过固化，产生一个不应期flag（这个应该在实例里面），抵消一次固化，应该跳过这次固化。然后，CD产生了固化。
也不允许本次查找中直接把AB-C固化成ABC，因为本质上，这次查找已经路过A1B了，现在是在走B1C，然后因为不应期跳过了一次固化。

注意，每次寻找时，应该找那些更长的。比如，下次剩下的字符串是CD，那么面对字典里面的C和CD，应该选择CD。

3) 范畴向量来源：是否使用外部词向量、手工分类轴，还是由用户交互维护？
暂时假设使用外部词向量，假设我们使用了腾讯的200维中文词向量库。

4) hubness 计算方法：(暂时当一个占位项，默认先全部置为无效，也就是不做任何“折寿”)

5）点亮衰减策略：先完全不采用衰减策略。

6背台背诵调度：完全不考虑。目前是实验性质。但是，我们希望的其实是另一种“调度”，也就是把一段文本先拆成词组，再拆成短句，再拆成长句，再一整段地，分别输入我们的系统。建议的方法是，每拆分一次，输入几次（具体几次？应该是直到收敛。也就是，终于有我们输入的固化元生成了。也就是我们循环输入AB，CD，EF，发现三个都注册了。那么就是收敛了然后我们输入ABCDEF，发现这个被固化注册了，那么就是收敛了。我们。当然，这是个倒序的进程，先拆成长句，短句，词组，然后反过来的顺序进行输入。我希望，有现存工具来帮我做拆分这件事情。也就是，背诵过程的输入是需要一个控制的。至于，比如，拆成短句要怎么输入？那自然是循环输入了。怎么定义短语，短句，长句，段落和“一次完整文本”？当然是收敛之后，那些我们输入的东西，就打一个标签。

7.输出给LLM 的排序：暂定9个关键短句，6个关键长句，3个可能相关的段落，1个相关记忆，2个可能相关的记忆。顺序都是按照点亮数，权重就是点亮次数排序而已。
是的，目前既然我们新增了“收敛”
这个规则，那么就应该每个收敛项（注意并不是所有的项都是收敛项）都挑选几个。

8.固化元容忍度“乘积规则”是否合理？：引入对数方法，不让容忍度过快速扩张。具体方法，你可以选一个自认合理的。我认为，可以用某种对数方法或者开方的方法去缓解并不合理的增速